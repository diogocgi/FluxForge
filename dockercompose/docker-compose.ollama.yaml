services:
  ollama:
    volumes:
      - ollama:/root/.ollama
    container_name: ollama
    tty: true
    restart: unless-stopped
    image: ollama/ollama:0.3.10
    environment:
      OLLAMA_HOST: "0.0.0.0"
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 30s
      retries: 5
      start_period: 10s
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - custom_network

  searxng:
    image: searxng/searxng:2024.6.7-f5eb56b63
    container_name: searxng
    ports:
      - "11435:8080"
    volumes:
      - ./searxng:/etc/searxng
    environment:
      SEARXNG_BIND_ADDRESS: "0.0.0.0"
      INSTANCE_NAME: "instance_searxng"
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"
    networks:
      - custom_network

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    volumes:
      - open-webui:/app/backend/data
    environment:
      # Searxng configuration
      ENABLE_RAG_WEB_SEARCH: "True"
      RAG_WEB_SEARCH_ENGINE: "searxng"
      RAG_WEB_SEARCH_RESULT_COUNT: 3
      RAG_WEB_SEARCH_CONCURRENT_REQUESTS: 10
      SEARXNG_QUERY_URL: "http://searxng:8080/search?q=<query>"
      WEBUI_AUTH: "false"

      # Ollama configuration
      OLLAMA_API_BASE_URL: "http://ollama:11434/api"
      OLLAMA_BASE_URL: "http://ollama:11434"

      #COMFYUI_BASE_URL: "http://127.0.0.1:8188"
      ENABLE_IMAGE_GENERATION: "True"
    depends_on:
      - ollama
      - searxng
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped
    ports:
      - "8081:8080"
    networks:
      - custom_network

volumes:
  ollama: {}
  open-webui: {}
  searxng: {}

networks:
  custom_network:
    driver: bridge
